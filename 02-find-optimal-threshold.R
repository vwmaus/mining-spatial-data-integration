# -----------------------------------------------------------------
# SCRIPT: 02-find-optimal-threshold.R
# -----------------------------------------------------------------
#
# This script is the primary "decision-making" part of the workflow. It analyzes the
# comprehensive `hcluster_concordance.csv` (generated by script 01) to determine the
# single best clustering strategy.
#
# Workflow:
# 1.  Calculates the "Global Optimum": It tests each threshold $h$ (e.g., 1-20 km)
#     against the *entire dataset*, finds the aggregate error (unassigned vs.
#     multi-assigned) for each $h$, and identifies the single best global threshold.
# 2.  Performs the "Local Optimum" (Meta-Optimization):
#     a.  It loops through a range of *maximum allowed* thresholds ($H_{limit}$).
#     b.  For each $H_{limit}$, it finds the *local-best* threshold ($h \le H_{limit}$)
#         for each individual `id_group`.
#     c.  It aggregates the errors from this "best-of-local" strategy to produce a
#         single error point for that $H_{limit}$.
#     d.  This process generates a "sensitivity curve" of the local approach.
# 3.  It finds the optimal point on this sensitivity curve to identify the
#     `optimal_max_local_threshold` ($H^*_{max}$), which represents the best
#     possible trade-off. This step is cached.
# 4.  Finally, it uses this $H^*_{max}$ to assemble the definitive set of clusters
#     and exports the final data release products (e.g., `cluster_features.gpkg`).


################################################################################
########################### REQUIRED PARAMETERS ################################
#
# --- Set a specific max local threshold ---
# Set to NULL to automatically find the optimum 'max_local_threshold'.
# Set to a number (e.g., 7) to *force* that threshold.
# This number MUST be within the 'sensitivity_range_km' defined below.
user_defined_max_local_threshold <- NULL # e.g., 7
#
# --- NEW: Caching Parameters ---
# Set to TRUE to force the script to re-calculate the sensitivity analysis
# even if a cached file exists.
force_rerun_sensitivity <- FALSE
#
# Path to store intermediate results
intermediate_results_path <- "./tmp/sensitivity_analysis_cache.rds"
#
# --- Sensitivity Analysis Range ---
# Define the range of 'max_local_threshold' values (in km) to test
sensitivity_range_km <- seq(1, 20, 1)
#
# --- Other Parameters ---
# Set the max global threshold (in km) to calculate the global optimum
# for comparison. This should match the max 'h' from script 01.
max_global_threshold <- 20
#
# concordance table to harmonise materials. Set to NULL to keep as they are
path_harmonisation_table <- "./data/harmonisation_all_materials.csv"
#
# set version of the data output (for naming the output folder)
release_version_name <- "all_materials"
release_version_date <- format(Sys.Date(), '%Y%m%d')
#
################################################################################

# Require libraries
library(sf)
library(stringr)
library(dplyr)
library(tibble)
library(readr)
library(tidyr)
library(ggplot2)
library(scales)
library(stringi)
library(purrr)
library(car)

# Plot colours
red_d <- "#cc3e5b"
green_d <- "#306056"
yellow_d <- "#f6ae2d" # Used for user-defined point
blue_d <- "#0072B2" # Used for meta-optimum

# --- Setup Output Path ---
path_output <- str_c("./output/", release_version_date, "-", release_version_name)
dir.create(path_output, recursive = TRUE, showWarnings = FALSE)
cat("Output will be saved to:", path_output, "\n")
dir.create("./tmp", showWarnings = FALSE) # Ensure ./tmp directory exists for cache

# --- Load Data ---
cat("Loading data...\n")
id_to_remove <- tibble(id=NULL)
if(file.exists("./data/incorrect_polygons_list.csv")) {
  id_to_remove <- read_csv("./data/incorrect_polygons_list.csv", show_col_types = FALSE)
}

# Renamed to 'hcluster_concordance' to match script 02's export logic
hcluster_concordance <- read_csv("./tmp/hcluster_concordance.csv", show_col_types = FALSE) |>
  left_join(
    st_read(dsn = "./tmp/cluster_data.gpkg", query = "SELECT id, area_mine FROM cluster_data", quiet = TRUE),
    by = "id"
  ) |>
  filter(!id %in% id_to_remove$id) # Polygon with error

# --- Harmonize Materials ---
if(!is.null(path_harmonisation_table) && file.exists(path_harmonisation_table)){
  cat("Harmonizing materials...\n")
  source("./R/harmonize_materials.R")
  mapping_table <- read_csv(path_harmonisation_table, show_col_types = FALSE)
  
  hcluster_concordance <- hcluster_concordance |>
    mutate(
      across(starts_with("primary_"), ~ harmonize_materials(.x, mapping_table, col_from = "material", col_to = "material_harmonized")),
      across(starts_with("list_"), ~ harmonize_materials(.x, mapping_table, col_from = "material", col_to = "material_harmonized"))
    )
} else {
  cat("Skipping harmonization.\n")
}

# --- Calculate Global Optimum (for comparison) ---
cat("Calculating global optimum point for reference...\n")
materials_area_global <- hcluster_concordance |>
  mutate(id_max_threshold := !!sym(str_c("id_hc_", max_global_threshold))) |>
  transmute(id, area_mine,
            across(starts_with("primary_"), ~ str_count(.x, ",") + 1, .names = "count{.col}"),
            across(starts_with("list_"), ~ str_count(.x, ",") + 1, .names = "count{.col}"),
            across(starts_with("primary_"), ~ is.na(.x))) |>
  select(-starts_with("list_")) |>
  pivot_longer(all_of(matches("primary|list")), names_to = c("var", "clust", "clust_dist"), values_to = "value", names_sep = "_") |>
  select(-clust) |>
  mutate(clust_dist = as.numeric(clust_dist)) |>
  filter(clust_dist <= max_global_threshold) |>
  pivot_wider(names_from = var, values_from = value) |>
  rename(unknown = primary)

pareto_optimal_point_global <- materials_area_global |>
  group_by(clust_dist) |>
  reframe(area_na = sum(area_mine, na.rm = TRUE),
          area_na = area_na / area_na,
          area_known = area_na * sum(area_mine * (!unknown), na.rm = TRUE),
          area_unknown = area_na * sum(area_mine * (unknown), na.rm = TRUE),
          area_multi_count = area_na * sum(area_mine * (countprimary - 1), na.rm = TRUE),
          area_mine = area_na * sum(area_mine, na.rm = TRUE),
          perc_area_known = area_known / area_mine,
          perc_area_unknown = area_unknown / area_mine,
          perc_area_multi_count = area_multi_count / area_mine
  ) |>
  mutate(distances = sqrt(perc_area_multi_count/max(c(perc_area_multi_count, 1e-12)))^2 + (perc_area_unknown/max(c(perc_area_unknown, 1e-12)))^2,
         pareto_index = clust_dist[which.min(distances)])

# Extract the single best global point
global_optimal_summary <- pareto_optimal_point_global |>
  filter(clust_dist == pareto_index) |>
  slice(1) |> # Get the first row (they are all the same)
  select(clust_dist, perc_area_unknown, perc_area_multi_count) |>
  mutate(approach = str_c("Global Optimum (at ", clust_dist, " km)"))


# --- Run Sensitivity Analysis Loop (with Caching) ---
cat("Checking for cached sensitivity analysis results...\n")
run_sensitivity_analysis <- TRUE
target_range <- sort(sensitivity_range_km)

if (file.exists(intermediate_results_path) && !force_rerun_sensitivity) {
  cat("  Found cached file. Checking if parameters match...\n")
  cached_results <- readRDS(intermediate_results_path)
  cached_range <- sort(unique(cached_results$max_local_thr_km))
  
  if (identical(cached_range, target_range)) {
    cat("  Cache parameters match. Loading results from file.\n")
    all_sensitivity_results <- cached_results
    run_sensitivity_analysis <- FALSE
  } else {
    cat("  Cache parameters do NOT match. Re-running analysis.\n")
  }
} else if (force_rerun_sensitivity) {
  cat("  'force_rerun_sensitivity' is TRUE. Re-running analysis.\n")
} else {
  cat("  No cached file found. Running analysis...\n")
}

if (run_sensitivity_analysis) {
  cat("Starting sensitivity analysis for max_local_threshold...\n")
  
  all_sensitivity_results <- purrr::map_dfr(sensitivity_range_km, ~ {
    
    current_max_local_thr <- .x
    cat("  Processing max_local_threshold =", current_max_local_thr, "km\n")
    
    col_name_max_thr <- str_c("id_hc_", current_max_local_thr)
    
    if (!col_name_max_thr %in% names(hcluster_concordance)) {
      cat("    ...skipping, column", col_name_max_thr, "not found in hcluster_concordance.\n")
      return(NULL)
    }
    
    # --- This logic is copied from 02-optimize-cluster-threshold.R (lines 100-153) ---
    
    materials_area_local <- hcluster_concordance |>
      mutate(id_max_threshold := !!sym(col_name_max_thr)) |>
      transmute(id, id_max_threshold, area_mine,
                across(starts_with("primary_"), ~ str_count(.x, ",") + 1, .names = "count{.col}"),
                across(starts_with("list_"), ~ str_count(.x, ",") + 1, .names = "count{.col}"),
                across(starts_with("primary_"), ~ is.na(.x))) |>
      select(-starts_with("list_")) |>
      pivot_longer(all_of(matches("primary|list")), names_to = c("var", "clust", "clust_dist"), values_to = "value", names_sep = "_") |>
      select(-clust) |>
      mutate(clust_dist = as.numeric(clust_dist)) |>
      filter(clust_dist <= current_max_local_thr) |>
      pivot_wider(names_from = var, values_from = value) |>
      rename(unknown = primary)
    
    pareto_optimal_point_local <- materials_area_local |>
      group_by(clust_dist, id_max_threshold) |>
      reframe(area_na = sum(area_mine, na.rm = TRUE),
              area_na = area_na / area_na,
              area_known = area_na * sum(area_mine * (!unknown), na.rm = TRUE),
              area_unknown = area_na * sum(area_mine * (unknown), na.rm =TRUE),
              area_multi_count = area_na * sum(area_mine * (countprimary - 1), na.rm = TRUE),
              area_mine = area_na * sum(area_mine, na.rm = TRUE),
              perc_area_known = area_known / area_mine,
              perc_area_unknown = area_unknown / area_mine,
              perc_area_multi_count = area_multi_count / area_mine
      ) |>
      group_by(id_max_threshold) |>
      mutate(distances = sqrt(perc_area_multi_count/max(c(perc_area_multi_count, 1e-12)))^2 + (perc_area_unknown/max(c(perc_area_unknown, 1e-12)))^2,
             pareto_index = ifelse(all(is.na(distances)), min(clust_dist), clust_dist[which.min(distances)])) |>
      ungroup()
    
    pareto_optimal_point_local_summary <- select(pareto_optimal_point_local, clust_dist, pareto_index, area_mine, area_known, area_unknown, area_multi_count) |>
      filter(clust_dist == pareto_index) |>
      summarise(across(starts_with("area"), ~sum(.x, na.rm = TRUE))) |>
      mutate(
        perc_area_known = area_known / area_mine,
        perc_area_unknown = area_unknown / area_mine,
        perc_area_multi_count = area_multi_count / area_mine,
        clust_dist = "",
        approach = str_c("Local (max threshold ", current_max_local_thr, " km)")
      )
    
    pareto_optimal_point_local_summary <- pareto_optimal_point_local_summary |>
      mutate(max_local_thr_km = current_max_local_thr)
    
    return(pareto_optimal_point_local_summary)
    
  }) # End of purrr::map_dfr

  cat("Sensitivity analysis complete.\n")
  
  # NEW: Save the results to cache
  saveRDS(all_sensitivity_results, file = intermediate_results_path)
  cat("Results cached to:", intermediate_results_path, "\n")
  
} # End of if(run_sensitivity_analysis)

# --- Find or Set Optimal Max Local Threshold ---
if (is.null(user_defined_max_local_threshold)) {
  # --- A: Automatic Discovery ---
  cat("Finding optimal 'max_local_threshold' from sensitivity curve...\n")
  
  optimal_local_run <- all_sensitivity_results |>
    mutate(
      distances = sqrt(perc_area_multi_count/max(c(perc_area_multi_count, 1e-12)))^2 + (perc_area_unknown/max(c(perc_area_unknown, 1e-12)))^2
    ) |>
    filter(distances == min(distances, na.rm = TRUE)) |>
    slice(1) # Take the first one if there's a tie
    
  optimal_max_local_threshold_km <- optimal_local_run$max_local_thr_km
  # plot_label <- str_c("Optimal Local Approach\n(Max Thr = ", optimal_max_local_threshold_km, " km)")
  plot_label <- str_c(optimal_max_local_threshold_km, " km (Optimum Max Local Thr)")
  plot_color <- blue_d
  
  cat("Optimal 'max_local_threshold' found:", optimal_max_local_threshold_km, "km\n")

} else {
  # --- B: User-Defined Threshold ---
  cat("Checking user-defined 'max_local_threshold'...\n")
  
  if (!user_defined_max_local_threshold %in% all_sensitivity_results$max_local_thr_km) {
    stop(paste("Error: User-defined threshold '", user_defined_max_local_threshold, 
               "km' is not in the tested 'sensitivity_range_km' (", 
               min(sensitivity_range_km), "to", max(sensitivity_range_km), ")."))
  }
  
  optimal_max_local_threshold_km <- user_defined_max_local_threshold
  
  # Get the data row corresponding to the user's choice for plotting
  optimal_local_run <- all_sensitivity_results |>
    filter(max_local_thr_km == optimal_max_local_threshold_km)
    
  # plot_label <- str_c("User-Defined Local Approach\n(Max Thr = ", optimal_max_local_threshold_km, " km)")
  plot_label <- str_c("User-Defined Max Local Thr = ", optimal_max_local_threshold_km, " km")
  plot_color <- yellow_d
  
  cat("Using user-defined 'max_local_threshold':", optimal_max_local_threshold_km, "km\n")
}


# --- Plot Sensitivity Results ---
cat("Generating sensitivity plot...\n")

global_points_for_plot <- pareto_optimal_point_global |>
  select(max_local_thr_km = clust_dist, perc_area_unknown, perc_area_multi_count) |>
  mutate(approach = "Global Approach Points")

global_optimum_point_for_plot <- global_optimal_summary |>
  select(max_local_thr_km = clust_dist, perc_area_unknown, perc_area_multi_count) |>
  mutate(approach = "Global Optimum")

gp <- ggplot(all_sensitivity_results, aes(x = perc_area_multi_count, y = perc_area_unknown)) +
  
  # Plot the Global approach trade-off curve (from script 02)
  geom_line(data = global_points_for_plot, aes(group = 1), color = "grey", linetype = "dashed") +
  geom_point(data = global_points_for_plot, aes(label = str_c(max_local_thr_km, " km")), color = "grey") +
  geom_text(data = global_points_for_plot,hjust=-0.1, vjust=-0.5, aes(label = str_c(max_local_thr_km, " km")), color = "grey") +
  
  # Plot the new Local approach sensitivity curve
  geom_line(aes(group = 1), color = "grey", linewidth = 1) +
  geom_point(aes(color = max_local_thr_km), size = 3) +
  #geom_text(aes(label = str_c(max_local_thr_km, " km")), vjust = -1, hjust = 0.5, size = 3, check_overlap = TRUE) +
  
  # Highlight the Global Optimum
  geom_point(data = global_optimum_point_for_plot, color = green_d, size = 5, shape = 18) +
  geom_text(data = global_optimum_point_for_plot, label = "Global Optimum", color = green_d, vjust = 1.5, hjust = -0.08) +
  
  # --- MODIFIED: Highlight the chosen (auto or user-defined) Local choice ---
  geom_point(data = optimal_local_run, color = red_d, size = 5, shape = 18, fill = red_d) +
  geom_text(data = optimal_local_run, label = plot_label, color = red_d, vjust = 0.4, hjust = -0.05) +
  
    # --- NEW: Add dashed lines and labels for Global Optimum ---
  geom_segment(data = global_optimum_point_for_plot, 
               aes(x = perc_area_multi_count, y = perc_area_unknown, xend = perc_area_multi_count, yend = 0), 
               linetype = "dashed", color = green_d) +
  geom_segment(data = global_optimum_point_for_plot, 
               aes(x = perc_area_multi_count, y = perc_area_unknown, xend = 0, yend = perc_area_unknown), 
               linetype = "dashed", color = green_d) +
  geom_text(data = global_optimum_point_for_plot, 
            aes(x = perc_area_multi_count, y = 0, label = scales::percent(perc_area_multi_count, accuracy = 1)), 
            color = green_d, vjust = 1.5, size = 5) +
  geom_text(data = global_optimum_point_for_plot, 
            aes(x = 0, y = perc_area_unknown, label = scales::percent(perc_area_unknown, accuracy = 1)), 
            color = green_d, hjust = 1.2, size = 5) +
            
  # --- NEW: Add dashed lines and labels for Local Optimum ---
  geom_segment(data = optimal_local_run, 
               aes(x = perc_area_multi_count, y = perc_area_unknown, xend = perc_area_multi_count, yend = 0), 
               linetype = "dashed", color = red_d) +
  geom_segment(data = optimal_local_run, 
               aes(x = perc_area_multi_count, y = perc_area_unknown, xend = 0, yend = perc_area_unknown), 
               linetype = "dashed", color = red_d) +
  geom_text(data = optimal_local_run, 
            aes(x = perc_area_multi_count, y = 0, label = scales::percent(perc_area_multi_count, accuracy = 1)), 
            color = red_d, vjust = 1.5, size = 5) +
  geom_text(data = optimal_local_run, 
            aes(x = 0, y = perc_area_unknown, label = scales::percent(perc_area_unknown, accuracy = 1)), 
            color = red_d, hjust = 1.2, size = 5) +
            
  scale_color_viridis_c(name = "Max Local Thr (km)", breaks = scales::pretty_breaks()) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_x_continuous(labels = percent_format(accuracy = 1), limits = c(0, 1)) +
  theme_minimal(base_size = 16) +
  labs(
    # title = "Sensitivity of Local Optimization to 'max_local_threshold'",
    # subtitle = "Each point on the red line represents the aggregated result of a full 'Local Optimization'\nrun using a different max_local_threshold parameter.",
    y = "Total Area Unassigned",
    x = "Total Area Assigned to Multiple Primary Commodities"
  ) +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.85, 0.75),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10)
  )

plot_filename <- str_c(path_output, "/fig-sensitivity-analysis-local-threshold.png")
ggsave(filename = plot_filename, plot = gp, bg = "#ffffff",
       width = 345, height = 140, units = "mm", scale = 1)

cat("Sensitivity plot saved to:", plot_filename, "\n")


# --- Re-run Local Optimization with the winning threshold ---
# This block generates the 'pareto_optimal_point_local' and 'materials_area_local'
# dataframes that are needed for the final export.

cat("Re-running final local optimization using optimal threshold of", optimal_max_local_threshold_km, "km...\n")

col_name_max_thr <- str_c("id_hc_", optimal_max_local_threshold_km)

materials_area_local <- hcluster_concordance |>
  mutate(id_max_threshold := !!sym(col_name_max_thr)) |>
  transmute(id, id_max_threshold, area_mine,
            across(starts_with("primary_"), ~ str_count(.x, ",") + 1, .names = "count{.col}"),
            across(starts_with("list_"), ~ str_count(.x, ",") + 1, .names = "count{.col}"),
            across(starts_with("primary_"), ~ is.na(.x))) |>
  select(-starts_with("list_")) |>
  pivot_longer(all_of(matches("primary|list")), names_to = c("var", "clust", "clust_dist"), values_to = "value", names_sep = "_") |>
  select(-clust) |>
  mutate(clust_dist = as.numeric(clust_dist)) |>
  filter(clust_dist <= optimal_max_local_threshold_km) |>
  pivot_wider(names_from = var, values_from = value) |>
  rename(unknown = primary)

pareto_optimal_point_local <- materials_area_local |>
  group_by(clust_dist, id_max_threshold) |>
  reframe(area_na = sum(area_mine, na.rm = TRUE),
          area_na = area_na / area_na,
          area_known = area_na * sum(area_mine * (!unknown), na.rm = TRUE),
          area_unknown = area_na * sum(area_mine * (unknown), na.rm = TRUE),
          area_multi_count = area_na * sum(area_mine * (countprimary - 1), na.rm = TRUE),
          area_mine = area_na * sum(area_mine, na.rm = TRUE),
          perc_area_known = area_known / area_mine,
          perc_area_unknown = area_unknown / area_mine,
          perc_area_multi_count = area_multi_count / area_mine
  ) |>
  group_by(id_max_threshold) |>
  mutate(distances = sqrt(perc_area_multi_count/max(c(perc_area_multi_count, 1e-12)))^2 + (perc_area_unknown/max(c(perc_area_unknown, 1e-12)))^2,
         pareto_index = ifelse(all(is.na(distances)), min(clust_dist), clust_dist[which.min(distances)])) |>
  ungroup()


# --- Assemble and Export Final Cluster Data ---
# This entire section is copied from '02-optimize-cluster-threshold.R'
# It will use the 'pareto_optimal_point_local' and 'materials_area_local'
# dataframes generated in the block above.

cat("Assembling final cluster concordance table...\n")

selected_threshold <- pareto_optimal_point_local |>
  filter(clust_dist == pareto_index) |>
  select(id_max_threshold, pareto_index)

local_ids <- hcluster_concordance |>
  mutate(id_max_threshold := !!sym(str_c("id_hc_", optimal_max_local_threshold_km))) |>
  select(id, id_max_threshold, starts_with('id_hc_')) |> # Keep all id_hc columns
  pivot_longer(all_of(matches("id_hc_")), names_to = c("var", "clust", "clust_dist"), values_to = "value", names_sep = "_") |>
  select(-c(var, clust)) |>
  mutate(clust_dist = as.numeric(clust_dist)) |> # Ensure clust_dist is numeric for join
  left_join(selected_threshold, by = "id_max_threshold") |>
  filter(clust_dist == pareto_index) |>
  select(id, id_cluster = value, pareto_index)

final_clusters <- select(materials_area_local, id, id_max_threshold, clust_dist) |>
  left_join(local_ids, by = "id") |>
  filter(clust_dist == pareto_index) |>
  group_by(id_max_threshold, id_cluster, pareto_index) |>
  mutate(id_cluster = str_c("H", str_pad(cur_group_id(), width = 7, pad = 0))) |>
  ungroup() |>
  select(id, id_cluster, dist_threshold = clust_dist)

final_cluster_concordance <- hcluster_concordance |>
  select(-starts_with("id_hc"), -id_group) |>
  pivot_longer(all_of(matches("primary|list")), names_to = c("var", "clust", "clust_dist"), values_to = "value", names_sep = "_") |>
  select(-clust) |>
  mutate(clust_dist = as.numeric(clust_dist)) |>
  pivot_wider(names_from = "var", values_from = "value") |>
  rename(primary_materials_list = primary, materials_list = list) |>
  left_join(final_clusters, by = "id") |>
  filter(clust_dist == dist_threshold) |>
  select(id, id_cluster, dist_threshold, primary_materials_list, materials_list, area_mine)

cat("Generating final plots and saving data release files...\n")

# --- Final Plot: Distribution of assigned materials ---
gp_dist_materials <- final_cluster_concordance |>
  mutate(n_primary = str_count(primary_materials_list, ","),
         n_materials = str_count(materials_list, ",") + 1) |>
  pivot_longer(c(n_primary, n_materials), names_to = "list", values_to = "n_materials") |>
  mutate(list = factor(list, c("n_materials", "n_primary"), c("All", "Primary"))) |>
  drop_na(n_materials) |>
  filter(n_materials > 0) |>
  ggplot(aes(x = n_materials, fill = list)) +
  geom_histogram(position = "identity", alpha = 0.5, binwidth = 1) +
  labs(x = "Number of assigned commodities",
       y = "Count of clusters") +
  scale_fill_viridis_d(name = "List of materials", option = "D", begin = 0, end = .7, direction = -1) +
  theme_minimal(base_size = 16) +
  theme(
    legend.position = "inside",
    legend.position.inside = c(.8, .8),
  )

ggsave(filename = str_c(path_output, "/fig-distribution-number-assigned-materials.png"), plot = gp_dist_materials, bg = "#ffffff",
       width = 140, height = 140, units = "mm", scale = 1)

# --- Final Plot: Distribution of optimal thresholds ---
gp_dist_thresholds <- final_clusters |>
  ggplot(aes(dist_threshold)) +
    geom_histogram(binwidth = 1, fill = green_d) +
    theme_minimal(base_size = 16) +
    labs(
      y = "Count of subclusters",
      x = "Cluster Threshold Distance (km)"
    ) 

ggsave(filename = str_c(path_output, "/fig-optimal-threshold-distribution-primary-materials.png"), plot = gp_dist_thresholds, bg = "#ffffff",
       width = 140, height = 140, units = "mm", scale = 1)

summary(final_clusters$dist_threshold)

# --- Final Data Export ---
cluster_features <- st_read("./tmp/cluster_data.gpkg") |>
  filter(!id %in% id_to_remove$id) |>
  select(id, data_source, id_data_source, data_source, area_mine) |>
  left_join(select(final_cluster_concordance, -area_mine), by = "id")  |>
  select(id, id_cluster, id_data_source, data_source, area_mine, primary_materials_list, materials_list, geom)

release_data <- select(cluster_features, -id_data_source)

st_write(release_data, dsn = str_c(path_output, "/cluster_features.gpkg"), layer = "mine_polygons", delete_dsn = TRUE, quiet = TRUE)
st_write(filter(release_data, str_detect(id, "A")), dsn = str_c(path_output, "/mine_polygons.gpkg"), layer = "mine_polygons", delete_dsn = TRUE, quiet = TRUE)
st_write(filter(release_data, str_detect(id, "P")), dsn = str_c(path_output, "/mine_points.gpkg"), layer = "mine_points", delete_dsn = TRUE, quiet = TRUE)
write_csv(final_cluster_concordance, str_c(path_output, "/mine_clusters.csv"))

cluster_features |>
  select(id, id_cluster, id_data_source, data_source, primary_materials_list, materials_list) |>
  write_csv(str_c(path_output, "/clusters_datasource_concordance.csv"))

cluster_features |>
  st_drop_geometry() |>
  as_tibble() |>
  filter(str_detect(id, "P")) |>
  select(id, id_cluster, id_data_source, data_source) |>
  write_csv(str_c(path_output, "/cluster_points_concordance.csv"))

cat("All processing and exporting complete.\n")